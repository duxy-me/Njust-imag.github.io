<!DOCTYPE html><html lang="en"><head><meta name="generator" content="React Static"/><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5, shrink-to-fit=no"/><link rel="preload" as="script" href="/templates/styles.e9e5bf98.js"/><link rel="preload" as="script" href="/templates/vendors~main.8ca25124.js"/><link rel="preload" as="script" href="/main.7753e46a.js"/><link rel="preload" as="style" href="/styles.92cee3d5.css"/><link rel="stylesheet" href="/styles.92cee3d5.css"/></head><body><div id="root"><div style="outline:none" tabindex="-1"><div class="App"><div class="jss2"><div class="jss3"><div class="jss4"><div class="content"><div><h3 class="MuiTypography-root MuiTypography-h3 MuiTypography-alignCenter" style="color:#57247b;height:80%">南京理工大学智能媒体分析实验室</h3></div><div><p class="MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom" style="color:#666666;margin-bottom:25px">Intelligent Media Analysis Group (IMAG), School of Computer Science, Nanjing University of Science and Technology</p></div></div></div></div><div class="jss6"><header class="MuiPaper-root MuiAppBar-root MuiAppBar-positionStatic MuiAppBar-colorPrimary jss9 MuiPaper-elevation4"><div class="MuiToolbar-root MuiToolbar-regular MuiToolbar-gutters"><div class="jss11"><div class="jss13"><div style="background-color:white"><a style="color:#57247b;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/">HOME</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/people">PEOPLE</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/news">NEWS</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/publication">PUBLICATIONS</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/competition">COMPETITIONS</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/grants">GRANTS</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/patent">PATENTS</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/honor">HONOR</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/resource">RESOURCE</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px;display:block" href="/contact">CONTACT</a></div></div></div></div></header></div><div class="jss5"><div style="outline:none" tabindex="-1"><div class="jss21"><h4 class="MuiTypography-root MuiTypography-h4 MuiTypography-alignCenter" style="padding-top:30px">NUS-wide-128</h4><div class="jss22"><h5 class="MuiTypography-root MuiTypography-h5 MuiTypography-alignLeft" style="padding-top:30px;padding-bottom:20px">Introduction</h5><div style="width:100%;height:2px;border-top:1px solid #999;clear:both"></div><p>NUW-WIDE-128: A Real-World Web Image Database. The dataset is based on <a href="http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm">NUS-WIDE</a> and includes:</p><div style="margin-bottom:150px"><p style="line-height:1.6;text-align:left"> <!-- -->(1) 269,648 images and the associated tags from Flickr, with a total number of 5,018 unique tags.</p><p style="line-height:1.6;text-align:left"> <!-- -->(2) Six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments and 500-D bag of words based on SIFT descriptions.</p><p style="line-height:1.6;text-align:left"> <!-- -->(3) Ground-truth for 128 concepts that can be used for evaluation.</p><p style="line-height:1.6;text-align:left"> <!-- -->(4) User information of 247,849 images.</p></div><h5 class="MuiTypography-root MuiTypography-h5 MuiTypography-alignLeft" style="padding-top:30px;padding-bottom:20px">Download</h5><div style="width:100%;height:2px;border-top:1px solid #999;clear:both"></div><div style="margin-bottom:150px"><p style="line-height:1.6;text-align:left"> <a href="../static/resource_file/ConceptsList.rar"><strong>ConceptsList.rar (downloads)</strong></a></p><p style="line-height:1.6;text-align:left"> <a href="../static/resource_file/Groundtruth.rar"><strong>Groundtruth.rar (downloads)</strong></a></p><p style="line-height:1.6;text-align:left"> <a href="https://pan.baidu.com/s/1kntqUK9a_OK31oZ_3rSbYQ"><strong>Flickr/image data</strong></a></p><p style="line-height:1.6;text-align:left"> <a href="../static/resource_file/ImageList.rar"><strong>ImageList.rar (downloads)</strong></a></p><p style="line-height:1.6;text-align:left"> <a href="../static/resource_file/imgLabes.rar"><strong>imgLabes.rar (downloads)</strong></a></p><p style="line-height:1.6;text-align:left"> <a href="https://pan.baidu.com/s/1yl-yywIYHsEgUcEf3LD0KA"><strong>Low_Level_Features</strong></a></p><p style="line-height:1.6;text-align:left"> <a href="../static/resource_file/NUS_WID_Tags.zip"><strong>NUS_WID_Tags.rar (downloads)</strong></a></p></div></div><h5 class="MuiTypography-root MuiTypography-h5 MuiTypography-alignLeft" style="padding-top:30px;padding-bottom:20px">Citation</h5><div style="width:100%;height:2px;border-top:1px solid #999;clear:both"></div><p>If you make use of the NUS-WIDE-128 dataset in any form, please cite the following reference.</p><div style="line-height:1.6;padding-bottom:5px"><p>@article{tang2017tri,<br>title={Tri-Clustered Tensor Completion for Social-Aware Image Tag Refinement},<br>author={Tang, Jinhui and Shu, Xiangbo and Qi, Guo-Jun and Li, Zechao and Wang, Meng and Yan, Shuicheng and Jain, Ramesh},<br>journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>year={2017},<br>doi={10.1109/TPAMI.2016.2608882},<br>publisher={IEEE}<br>}</p></div><div style="line-height:1.6;padding-bottom:20px"><p>@article{tang2016generalized,<br>title={Generalized Deep Transfer Networks for Knowledge Propagation in Heterogeneous Domains},<br>author={Tang, Jinhui and Shu, Xiangbo and and Li, Zechao and Qi, Guo-Jun and Wang, Jingdong},<br>journal={ACM Transactions on Multimedia Computing, Communications, and Applications},<br>volume = {12},<br>number = {4s},<br>year={2016}<br>}</p></div></div></div></div></div></div><div class="jss7"><div class="jss8"><div><p class="MuiTypography-root MuiTypography-body1" style="color:#ffffff">Copyright © 2022 南京理工大学智能媒体分析实验室. All rights reserved.</p></div><div><p class="MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom" style="color:#ffffff">Theme: ColorMag by ThemeGrill. Powered by WordPress.</p></div></div></div></div></div><script type="text/javascript">window.__routeInfo = JSON.parse("{\"template\":\"__react_static_root__/src/containers/NUS-wide\",\"sharedHashesByProp\":{},\"data\":{},\"path\":\"resource/NUS-wide\",\"sharedData\":{},\"siteData\":{}}");</script><script defer="" type="text/javascript" src="/templates/styles.e9e5bf98.js"></script><script defer="" type="text/javascript" src="/templates/vendors~main.8ca25124.js"></script><script defer="" type="text/javascript" src="/main.7753e46a.js"></script></body></html>